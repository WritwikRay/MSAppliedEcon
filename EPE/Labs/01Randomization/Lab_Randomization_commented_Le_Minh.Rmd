---
title: "Lab Session: Randomization"
author: "Cristian Huse"
output: pdf_document
comment(question of students from the R session): "Le Minh Hoang"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Converted to R in May/October 2021 using RStudio Version 1.4.1106
```

# Introduction

This lab session is the first using the material from IEP (Gertler et al 2016), which uses the fictionalized case study of the HISP dataset. Please see Appendices A and B for details. While the original implementation relies on Stata, in what follows we will conduct our analysis in R. Nevertheless, although there are some departures, alternative analyses or generalizations, please note that the R-based sessions still draw substantially on the original material.

Comment from Minh: the Health Insurance Subsidy Program (HISP)'s objective is to reduce the burden of health-related out-of-pocket expenditures for low income households 

This particular lab session deals with **randomization**. The structure of the session is as follows:

* Preparation;
  + Load libraries, setwd, load data (.dta format)

* Example 1. Estimating **treatment effects with random assignment**
  + Compare treatment and control (at the locality level) **post-treatment**

* Example 2. Testing for **balance in baseline outcome**
  + Compare treatment and control **pre-treatment** (data which usually isn't available)
  
* Example 3. Testing **balance in baseline covariates**
  + We focus on age, but the general idea is to compare whether treatment and control are comparable pre-treatment for all covariates (there is usually a table reporting these results on a paper)

* Example 4. Estimating treatment effects **controlling for household characteristics**
  + This goes back to the first estimation of treatment effects but offers some advantages



# Preparation

```{r, echo=F}

## Preparation ####
##Install packages (I attempted to list all packages needed in the basic lab sessions)
# install.packages("AER")
# install.packages("clubSandwich")
# install.packages("compositions")
# install.packages("fixest")
# install.packages("haven")
# install.packages("MatchIt")
# install.packages("Matching")
# install.packages("modelsummary")
# install.packages("optmatch")
# install.packages("panelr")
# install.packages("plm")
# install.packages("stargazer")
# install.packages("tidyverse")
# install.packages("tidyr")

```

```{r, message=F}
## Initialize ####

rm(list=ls())

#Load libraries
library(haven) #for read_dta
library(modelsummary)

## Set working directory
#setwd("INSERT PATH OF FOLDER WHERE YOU SAVED THE DATASET WITH / or\\")

current_rmd <- rstudioapi::getSourceEditorContext()$path

# Set the working directory to the location of the R Markdown file
setwd(dirname(current_rmd))


## open data
#Open the cleaned data set
#set path for data
evaluation <- file.path(getwd(), "Data", "evaluation.dta")

#import .dta file
evaluation.df <- read_dta(evaluation)

```



# Randomized Assignment

In this context, the program is randomized at the village level, and you compare follow-up situation of eligible households in treatment and comparison villages. That is, compare only among eligible.

```{r}
#Select the relevant data, i.e., keep if eligible == 1
random.df = subset(evaluation.df, eligible == 1)


```



# Example 1. Randomized Assignment in a Regression Framework (Linear Regression)

Under randomized assignment, one compares the effect of treatment $P$ (treatment_locality, treatment is at the locality level) on the outcome of interest $Y$ (health expenditures). To do so, one compares the outcome for individuals in treatment and control groups, as detailed in the lecture. In regression notation:

$$ Y_i = \alpha + \delta P_i + \varepsilon_i $$

where the estimated regression coefficient of $\delta$ provides an estimate of the ATE (the difference in average outcomes between the treated and non-treated subsamples) and the error term $\varepsilon$ captures individual factors that may affect the relationship between the program and the outcome. (Please see the lecture for details.)

To implement this, *"we regress our outcome of interest (health expenditures) on a binary treatment variable (treatment_locality), which is equal to 1 if the household is located in a treatment area. We run this regression on the sample of households that were eligible for HISP and based on data collected after the program has been administered (round = 1). The estimated regression coefficient for $\delta$ is -10.14, indicating that eligible households exposed to HISP spent $10.14 less on health expenditures than eligible households in the comparison group. The standard error of the coefficient is 0.396. The t-statistic calculated in the regression, -25.63, shows that this coefficient is statistically significant at the 1 percent level."*

That is, individuals are compared only once treatment was applied, i.e., follow-up, denoted by **round == 1**. Given the potential within locality cross-correlation, standard errors are clustered at the locality level. (that is, there might be a chance where people in a certain locality might get being treated more) 



```{r, results='asis'}

ex1_lm <- lm(health_expenditures ~ treatment_locality, round == 1, data = random.df) # we are looking at the follow up
summary(ex1_lm)

ex1_feols_1 <- feols(health_expenditures ~ treatment_locality, subset = ~ round==1, data = random.df)
ex1_feols_2 <- feols(health_expenditures ~ treatment_locality, subset = ~ round==1, cluster = ~locality_identifier, data = random.df)
summary(ex1_feols_1)
summary(ex1_feols_2)

#question of students: why the t-statistic mentioned above is -49.15 and in the summary of model 1 it's only -25.6. Answer: the ex1_lm does take into account of the locality, so the t-statistic is -25.6, the ex1_feols_1 model does not, so it's -49.15

#tmp.df = subset(evaluation.df, eligible == 1 & round ==1)
#plot(tmp.df$treatment_locality, tmp.df$health_expenditures)

model1 <- list("(1) TE w/ Randomization" = ex1_lm)

modelsummary(model1, vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, gof_omit = "AIC|BIC|Log.Lik.")

```

\pagebreak

# Example 2. Testing for Balance in a Baseline Outcome

*"Independence of potential outcomes is one of the most crucial assumptions to ensure that the difference in average outcomes between program beneficiaries and non beneficiaries provides a consistent estimate of the average treatment effect. While this assumption cannot generally be verified, some falsification tests can be implemented to identify cases when it does not hold."*

Assuming pre-programme data is available, there should be **no measurable effect** between treated and non-treated individuals **before** the programme's introduction (**round == 0**). In regression notation, with $t=0$ being the pre-treatment period:

$$ Y_{i,t=0} = \alpha + \delta P_i + \varepsilon_i $$

*"When the baseline outcome is regressed on a binary variable capturing exposure to treatment, the estimated coefficient is very small (-0.084) and not statistically significant (p value of 0.693). This indicates that eligible households in the treatment and comparison groups have similar levels of health expenditures prior to the intervention."*

This type of analysis is sometimes referred to as a **placebo** -- one should not see an effect of the programme prior to its introduction.



```{r, results='hold'}

ex2_lm <- lm(health_expenditures ~ treatment_locality, 
             round == 0, data = random.df)

model2 <- list("(2) Balance in Baseline Outcome" = ex2_lm)

modelsummary(model2, vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, gof_omit = "AIC|BIC|Log.Lik.")

```

\pagebreak

# Example 3. Testing Balance in a Baseline Covariate

In a similar vein as in Example 2, one can check balancing, i.e., check whether characteristics are similar across treated and non-treated individuals pre-treatment (**round == 0**). Denoting a characteristic by $X$, balance in this other observed pre-program characteristic can be estimated using the following regression;

$$ X_{i,t=0} = \alpha + \delta P_i + \varepsilon_i $$

where the change from Example 2 is the replacement of $Y$ with $X$.



```{r, results='hold'}

ex3_lm <- lm(age_hh ~ treatment_locality, 
             round == 0, data = random.df) #characteristic in this case is age_hh

model3 <- list("(3) Balance in Baseline Covariate (age)" = ex3_lm)

modelsummary(model3, vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, gof_omit = "AIC|BIC|Log.Lik.")

#### education of spouse
ex3_lm_edu_spouse <- lm(educ_sp ~ treatment_locality, 
             round == 0, data = random.df) #characteristic in this case is age_hh

model3 <- list("(3) Balance in Baseline Covariate (education of spouse)" = ex3_lm_edu_spouse)

modelsummary(model3, vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, gof_omit = "AIC|BIC|Log.Lik.")

```

\pagebreak

# Example 4. Randomized Assignment in a Regression Framework (Multivariate Regression)

Finally, we can control for characteristics of treatment and control groups in a regression framework, pre-treatment  (**round == 1**). This is directly comparable to Example 1 above.

The estimated regression coefficient for $\delta$ -10.01, implying that households in a treated locality $10.01 less on health expenditures than those in localities not exposed to the program -- **holding all the control variables constant**. The t-statistic calculated in the regression shows that this coefficient is statistically significant at the 1 percent level.



```{r, results='hold'}

ex4_lm <- lm(health_expenditures ~ treatment_locality + age_hh + age_sp + educ_hh +
               educ_sp + female_hh + indigenous + hhsize + dirtfloor + bathroom + land +
               hospital_distance, 
             round == 1, data = random.df)

model4 <- list("(4) TE w/ Controls" = ex4_lm)

modelsummary(model4, vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, gof_omit = "AIC|BIC|Log.Lik.")

```

\pagebreak

# Comparing Estimates

We now report the results of Examples 1 and 4, but one could have one table for placebo effects, one for covariate balance etc.



```{r, results='hold'}

modelsummary(c(model1,model4), vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, gof_omit = "AIC|BIC|Log.Lik.")

#Question by the student, why the intercept of the model with control variables is significantly higher? Because of the control for the whole set of variable. This isn't the major concern, the key parameter is really the treatment effect, which is very robust
```


# References

Gertler, Paul J.; Martinez, Sebastian; Premand, Patrick; Rawlings, Laura
B.; Vermeersch, Christel M. J. (2016). Impact Evaluation in Practice, Second Edition, Technical Companion (Version 1.0). Washington, DC: Inter-American Development Bank and World Bank.



\pagebreak

# Appendix A. IEP's Technical Companion

![Technical Companion, p. 1](Figs/IEP_TC_p1.jpg){width=70%}

\pagebreak

# Appendix B. IEP's Technical Companion Read Me: Dataset Description

Please make sure to return to this description also in future labs so you understand how the data is being "cut" according to the method under study.

![Technical Companion Read Me, p. 1](Figs/IEP_TC_README1.jpg){width=70%}

![Technical Companion Read Me, p. 2](Figs/IEP_TC_README2.jpg){width=70%}



# QUIZ QUESTIONS
1.	The difference between internal and external validity
a.	Internal validity: the observed effect of the dependent variable is due to the treatment and not something else
b.	External validity: the sample of the study accurately represents the society 

2.	The law of large numbers: 
a.	If the sample size is big enough (normally larger or equal to 30): then the mean of the sample will approach the mean of the real population

3.	The concept of two-stage randomization: 
a.	First take a randomized sample from the population 
b.	The sample then is assigned randomly into the control group and the treatment group
Why it is a good strategy: 
-	It eliminates the problem of selection bias and heterogeneous treatment problem
-	It assures the Independence Assumption 

4.	The pitfalls of randomization: 
a.	Non-compliance: people who are eligible for the treatment donâ€™t want to get the treatment
b.	Attrition: it is difficult to observe the outcome of the treatment 
c.	Hawthorne effect: unit who is being observed change the behaviour
d.	John Henry effect: the control group try to work harder to get compensate
