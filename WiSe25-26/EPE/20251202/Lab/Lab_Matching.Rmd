---
title: "Lab Session: Matching"
author: "Cristian Huse"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Converted to R in May/October 2021 using RStudio Version 1.4.1106
```

# Introduction

This lab session is about **Matching**. The structure of the session is as follows:

* Preparation;
  + Load libraries, setwd, load data (.dta format)

* Example 13. Propensity Score Matching (PSM) Estimates
  + Here, we estimate a probit model and compare matched individuals

* PSM using the Nearest-Neighbour Method
  + 1-NN, 2-NN, and 1-NN with logit instead of probit model

* PSM Using Full Matching

Please note that there are different libraries implementing propensity score matching and we selectively cover only one of them.


# Preparation

```{r, echo=F}

##Install packages (I attempted to list all packages needed in the basic lab sessions)
#install.packages("AER")
#install.packages("clubSandwich")
#install.packages("compositions")
#install.packages("fixest")
#install.packages("haven")
#install.packages("MatchIt")
#install.packages("Matching")
#install.packages("modelsummary")
#install.packages("optmatch")
#install.packages("panelr")
#install.packages("plm")
#install.packages("stargazer")
#install.packages("tidyverse")
#install.packages("tidyr")

```

```{r, message=F}
## Initialize ####

rm(list=ls())

#Load libraries
library(AER) #for ivreg
library(fixest) #for feols etc
library(haven) #for read_dta
library(Matching)
library(MatchIt)
library(modelsummary) #for neat tables
library(optmatch)
library(panelr)
library(tidyverse)

```

```{r, echo=F}
## Initialize ####
## Set working directory
#Specify the access path to the computer folder you will use for the analysis
#setwd("INSERT PATH TO PROJECT FOLDER WITH / or \\")  
setwd("C:/Users/huse-admin/Dropbox/CRISTIAN/Teaching/Cursos_Meus/Teaching_2021/EPE/Lab7_Matching")

rm(list=ls())

## open data
#Open the cleaned data set
#set path for data
evaluation <- file.path(getwd(), "Data", "evaluation.dta")
#import .dta file
evaluation.df <- read_dta(evaluation)

```



# Propensity Score Matching (PSM)

Assume we have a group of treated and another of non-treated individuals for which we observe a number of characteristics $X_i$ at baseline, i.e., before the programme is rolled out. One way to quantify treatment effects would be to estimate a model of the form

$$ Y_i = \alpha + \delta P_i + \gamma X_i + \varepsilon_i $$

This strategy would provide a causal estimate for the effect of the program only in very particular cases and might suffer from sample selection, omitted variable bias, and other problems (recall the FTCI). 

Matching parallels the standard regression approach by matching observations in treatment and control groups which are "similar" in terms of characteristics. To the extent that observed characteristics are the drivers of heterogeneity (as opposed to unobserved heterogeneity), matching is a good strategy, being often combined with other quasi-experimental methods such as DD. (Note: Matching per se doesn't ensure causal estimates, but is a way to match to a, say, treated subject, its "closest" non-treated subjects.)

PSM produces a (propensity) score using a regression of programme participation on a set of observed (pre-policy) covariates. Then units in the treatment and comparison group with the closest propensity scores are matched, and differences in outcomes are calculated within each matched pair. The matching
procedure is then repeated for all individuals in the treatment group, and averages in differences in outcomes within pairs are computed. 

There exist different methods to calculate the “closest match” (see slides). 


# Example 13. Propensity Score Matching Estimates

This example illustrates how to implement PSM in the HISP data. First, estimate a probit (alternatively, logit) model where the dependent variable is an indicator of programme participation and the covariates are individual characteristics **pre-programme**. The output of the probit regression shows which variables are the strongest predictors of program participation. 

Before performing any estimation, one needs to **reshape** the data. The intuition is as follows. In the **long format**, each household identifier (the id) appears in two rounds (the waves, 0 and 1), i.e., it is computed as if observed twice (pre- and post-treatment). What we want to match individuals on is on pre-programme characteristics, therefore the reshape operation. Please make sure to have a look at the data before and after reshaping it so you understand what you are doing.

```{r}
#reshape the database
#?panelr::panel_data
# Informs who are i (id) and t (wave)
#?panelr::widen_panel
# Will make round "disappear"
# Note it may take some time for this reshape command to run

eval_wide.df <- panel_data(evaluation.df, id = household_identifier, wave = round) %>%
  widen_panel(separator = "_") %>%
  subset(select = c(-age_hh_1, -age_sp_1, -educ_hh_1, -educ_sp_1, -hospital_1)) %>%
  rename(age_hh = age_hh_0, age_sp = age_sp_0, educ_hh = educ_hh_0, 
         educ_sp = educ_sp_0, hospital = hospital_0)

```


Recall that we are comparing health expenditures at follow-up (post-policy) between treated individuals (enrolled households) and a set of matched non-treated individuals (non-enrolled households) from both treatment and control villages.

The probit estimates highlight the significance of variables such as household size and dwellings with dirt floors in predicting participation in HISP. The propensity score is obtained by computing the predicted values from this first stage. 

```{r}

ex13_glm <- glm(enrolled ~ age_hh + age_sp + educ_hh + educ_sp + female_hh + 
                  indigenous + hhsize + dirtfloor + bathroom + land + 
                  hospital_distance, 
                family = binomial(link = "probit"), 
                data = eval_wide.df)

modelsummary(ex13_glm, 
             vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, 
             gof_omit = "AIC|BIC|Log.Lik.|R2 Adj.|R2 Within|R2 Pseudo|F")


```



The following figure plots the propensity scores for the treatment and control groups. Once the propensity scores are calculated, the next step is to find for each individual in the treatment group, a comparable “match” with a similar propensity score in the comparison group, i.e., an individual with characteristics such that its likelihood of participating in the program are the same as the treatment individual for which a match is sought. 

Therefore, matching generally occurs in the area of **common support** where the propensity scores for the treatment and comparison groups overlap. Therefore it is important to check to which extent the **common support assumption** holds empirically.

Once matching is performed, differences in outcomes within pairs are computed and the averages of these differences are obtained to provide estimates of treatment effects. A technical but important note is that matching will typically recover the ATT (Average Treatment Effect on the Treated) instead of the ATE. Intuitively, the ATE would be estimable if one were not to discard observations from the sample or if one were to target full sample balance (dropping observations would go against the idea of a random sample, on which the ATE is intuitively based). Therefore, one should not get surprised with the default option of the type **estimand = "ATT"** in Matching packages.

```{r}
#drop observations omitted in probit estimation, then append fitted values
eval_wide.df <- subset(eval_wide.df, 
                       household_identifier != 
                         subset(eval_wide.df,select=-hospital)
                       [!complete.cases(subset(eval_wide.df, 
                                               select = -hospital)),]$household_identifier) %>%
  mutate(pscore = ex13_glm$fitted.values)

# Density estimates separately for subsamples of enrolled and non-enrolled
  eval_wide_1.df <- subset(eval_wide.df, enrolled == 1)
  eval_wide_0.df <- subset(eval_wide.df, enrolled == 0)
  ex13_density1 <- density(eval_wide_1.df$pscore)
  ex13_density0 <- density(eval_wide_0.df$pscore)

  # Collect x (possibly a grid of values instead of the actual data) and y estimates
  take1 <- ex13_density1$x
  den1 <- ex13_density1$y
  take0 <- ex13_density0$x
  den0 <- ex13_density0$y
 
# Plot
  plot(take0, den0, type = "l", xlim = c(0, 0.8), ylim = c(0, 4), 
       xlab = "Pr(enrolled)", ylab = "density: Pr(enrolled)", 
       lty = 1)
  lines(take1, den1, col = "red", lty = 2)
  legend("topright",
         legend = c("density: Pr(enrolled=0)", "density: Pr(enrolled=1)"), 
         col = c("black", "red"), lty = c(1, 2), )

```



# PSM Using the Package [\textcolor{blue}{MatchIt}](https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html)

## [\textcolor{blue}{Checking for Balance}](https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html#check-initial-imbalance)

We start estimating a probit and checking balance. To do so, we specify the "first-stage" equation (**link = "probit"**) and call the library to check balance. The key statistics to focus on are **Std. Mean Diff.**, **Var. Ratio**, and **eCDF**. One should aim for values close to zero for the first and third, and values close to one for the middle one as measures of good balance.

```{r}
# Notation
Y <- eval_wide.df$health_expenditures_1
Tr <- eval_wide.df$enrolled

# Note: method = NULL 
m.out0 <- matchit(Tr ~ age_hh + age_sp + educ_hh + educ_sp + female_hh + indigenous +
                    hhsize + dirtfloor + bathroom + land + hospital_distance, 
                  distance = "glm", 
                  link = "probit", 
                  method = NULL, 
                  data = eval_wide.df)

summary(m.out0)

```

## [\textcolor{blue}{Matching Methods}](https://cran.r-project.org/web/packages/MatchIt/vignettes/matching-methods.html)

There is a variety of matching methods, whose technical study is outside the scope of this session. Classical ones include **Nearest Neighbour Matching (method = "nearest")** and **Caliper matching (caliper)**, see link above for discussion and details. Intuitively, NN selects the $K$ observations closest to a given element of the treatment group (K is chosen by the econometrician) whereas caliper matching selects all observations $k$ such that they are within a radius $r$ a given element of the treatment group (formally, $k$ such that $||x_k - x_0|| < r$ for each $x_0$ belonging to the treatment group, $d$ chosen by the econometrician).

In what follows, we first perform the classical 1:1 NN matching method (one element of the control group is assigned to an element of the treatment group) and assess balance. We then specify a regression using the object generated by the **MatchIt** package as input in the next step -- in practice, it carries with it the matched pairs. Note that there are several options available when it comes to matching and that both syntax and output make it conformable with **modelsummary**.

Looking at the diagnostics, extreme propensity scores are more scarce than intermediate ones, as expected. Comparing eQQ plots we get a sense that the matching procedure is doing a reasonably good job, at least for the three selected variables (age_hh, hhsize, hospital_distance) -- observations of treated vs control units are mostly within the confidence bounds. You might want to examine QQ plots of other variables, especially those for which the diagnostics **Std. Mean Diff.**, **Var. Ratio** or **eCDF** raise concerns.

Finally, note that while the estimate doesn't exactly nail the original ones, at $-\$10.02$ their values are pretty close (regard the remark on ATE vs. ATT, which is the estimand used here). 

```{r}

m.out1 <- matchit(Tr ~ age_hh + age_sp + educ_hh + educ_sp + 
                    female_hh + indigenous + hhsize + dirtfloor + 
                    bathroom + land + hospital_distance, 
                  data = eval_wide.df,
                  method = "nearest", distance = "glm", 
                  link = "probit")

m.out1

# Tools for assessing balance
summary(m.out1, un = FALSE)
plot(m.out1, type = "jitter", interactive = FALSE)
plot(m.out1, type = "qq", interactive = FALSE,
     which.xs = c("age_hh", "hhsize", "hospital_distance"))

m.data1 <- match.data(m.out1)

# Specify the regression
fit1 <- lm(health_expenditures_1 ~ enrolled + 
             age_hh + age_sp + educ_hh + educ_sp + 
             female_hh + indigenous + hhsize + dirtfloor +
             bathroom + land + hospital_distance, 
           data = m.data1, 
           weights = weights)

#coeftest(fit1, vcov. = vcovCL, cluster = ~locality_identifier)

model1 <- list("(1) 1-NN" = fit1)

modelsummary(model1,
             vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, 
             gof_omit = "AIC|BIC|Log.Lik.|R2 Adj.|R2 Within|R2 Pseudo|F")

```

As an alternative, we will adjust the NN-method setting so now two (**ratio = 2**) instead of only one neighbour are selected for each treated observation. (You can check yourself that if you select **ratio = 3**, not every observation will get three neighbours). Note that you can experiment with a number of additional settings which we won't be cover, e.g., **distance**.

When looking at the diagnostics, you will notice that the QQ plots don't necessarily look better than the previous ones for the variables selected. Moreover, at $-\$10.0$, the treatment effect hardly changes when compared to the previous case.

```{r}

m.out2 <- matchit(Tr ~ age_hh + age_sp + educ_hh + educ_sp + 
                    female_hh + indigenous + hhsize + dirtfloor + 
                    bathroom + land + hospital_distance, 
                  data = eval_wide.df,
                  method = "nearest", distance = "glm", 
                  link = "probit", 
                  ratio = 2)

m.out2

# Tools for assessing balance
summary(m.out2, un = FALSE)
plot(m.out2, type = "jitter", interactive = FALSE)
plot(m.out2, type = "qq", interactive = FALSE,
     which.xs = c("age_hh", "hhsize", "hospital_distance"))

m.data2 <- match.data(m.out2)

# Specify the regression
fit2 <- lm(health_expenditures_1 ~ enrolled + 
             age_hh + age_sp + educ_hh + educ_sp + 
             female_hh + indigenous + hhsize + dirtfloor +
             bathroom + land + hospital_distance, 
           data = m.data2, 
           weights = weights)

#coeftest(fit2, vcov. = vcovCL, cluster = ~locality_identifier)

model2 <- list("(2) 2-NN" = fit2)

modelsummary(model2,
             vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, 
             gof_omit = "AIC|BIC|Log.Lik.|R2 Adj.|R2 Within|R2 Pseudo|F")

```

Another moving part when performing matching is the regression model used in the first-stage regression. Therefore, we now investigate what happens when one replaces a probit with a logit model in Specification (1). These models differ when it comes to the underlying distributional assumption, with the probit model assuming a Normal distribution and the logit model assuming a Type-1 extreme value distribution. (We will discuss those models in detail in the future.)

Looking at the results, the estimates are very close when comparing those models ($-10.02$ vs. $-10.18$), which is not uncommon, at least in this specific time of model (binary choice).

```{r}

m.out3 <- matchit(Tr ~ age_hh + age_sp + educ_hh + educ_sp + 
                    female_hh + indigenous + hhsize + dirtfloor + 
                    bathroom + land + hospital_distance, 
                  data = eval_wide.df,
                  method = "nearest", distance = "glm", 
                  link = "logit")

m.out3

# Tools for assessing balance
summary(m.out3, un = FALSE)
plot(m.out3, type = "jitter", interactive = FALSE)
plot(m.out3, type = "qq", interactive = FALSE,
     which.xs = c("age_hh", "hhsize", "hospital_distance"))

m.data3 <- match.data(m.out3)

# Specify the regression
fit3 <- lm(health_expenditures_1 ~ enrolled + 
             age_hh + age_sp + educ_hh + educ_sp + 
             female_hh + indigenous + hhsize + dirtfloor +
             bathroom + land + hospital_distance, 
           data = m.data3, 
           weights = weights)

#coeftest(fit3, vcov. = vcovCL, cluster = ~locality_identifier)

model3 <- list("(3) 1-NN w/ Logit" = fit3)

modelsummary(model3,
             vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, 
             gof_omit = "AIC|BIC|Log.Lik.|R2 Adj.|R2 Within|R2 Pseudo|F")

```



Finally, we will try a different matching method such as **full matching**, which matches every treated unit to at least one control and every control to at least one treated unit. The treatment effect in this case is lower yet close to previous estimates, at $-\$9.83$. (Please note that this computation can take longer than the previous one.)

All in all, we can say that the programme effects are largely robust to the methods used.


```{r}

m.out4 <- matchit(Tr ~ age_hh + age_sp + educ_hh + educ_sp + 
                    female_hh + indigenous + hhsize + dirtfloor + 
                    bathroom + land + hospital_distance, 
                  data = eval_wide.df,
                  method = "full", distance = "glm", 
                  link = "probit")

m.out4

# Tools for assessing balance
summary(m.out4, un = FALSE)
plot(m.out4, type = "jitter", interactive = FALSE)
plot(m.out4, type = "qq", interactive = FALSE,
     which.xs = c("age_hh", "hhsize", "hospital_distance"))

m.data4 <- match.data(m.out4)

# Specify the regression
fit4 <- lm(health_expenditures_1 ~ enrolled + 
             age_hh + age_sp + educ_hh + educ_sp + 
             female_hh + indigenous + hhsize + dirtfloor +
             bathroom + land + hospital_distance, 
           data = m.data4, 
           weights = weights)

#coeftest(fit3, vcov. = vcovCL, cluster = ~locality_identifier)

model4 <- list("(4) Full Matching" = fit4)

modelsummary(model4,
             vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, 
             gof_omit = "AIC|BIC|Log.Lik.|R2 Adj.|R2 Within|R2 Pseudo|F")

```

We end by comparing all specifications we estimated in this session. The main result is that the estimates are largely robust across models.

```{r}

modelsummary(c(model1, model2, model3, model4),
             vcov = ~ locality_identifier, 
             stars = c("*" = .1, "**" = .05, "***" = .01), 
             fmt = 3, 
             gof_omit = "AIC|BIC|Log.Lik.|R2 Adj.|R2 Within|R2 Pseudo|F")

```



# References

Gertler, Paul J.; Martinez, Sebastian; Premand, Patrick; Rawlings, Laura
B.; Vermeersch, Christel M. J. (2016). Impact Evaluation in Practice, Second Edition, Technical Companion (Version 1.0). Washington, DC: Inter-American Development Bank and World Bank.
